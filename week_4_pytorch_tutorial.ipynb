{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 4: Introduction to PyTorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f497a46de4a2e5c"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:14.857956200Z",
     "start_time": "2023-08-21T01:14:14.826684600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the dataset, split into input (X) and output (y) variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e390cfed2c55e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:14.904811900Z",
     "start_time": "2023-08-21T01:14:14.842311300Z"
    }
   },
   "id": "6717ade40a01c2a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But these data should be converted to PyTorch tensors first. One reason is that PyTorch\n",
    "usually operates in a 32-bit floating point while NumPy, by default, uses a 64-bit floating point. Mix-and-match is not allowed in most operations. Converting to PyTorch tensors can avoid the implicit conversion that may cause problems. You can also take this chance to correct the shape to fit what PyTorch would expect, e.g., prefer n×1 matrix over n-vectors.\n",
    "\n",
    "**To convert, create a tensor out of NumPy arrays:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3070d4d699565ff3"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:14.904811900Z",
     "start_time": "2023-08-21T01:14:14.857956200Z"
    }
   },
   "id": "69b32a33a54b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You are now ready to define your neural network model.\n",
    "A model can be defined as a sequence of layers. You create a Sequential model with the\n",
    "layers listed out. The first thing you need to do to get this right is to ensure the first layer has the correct number of input features. In this example, you can specify the input dimension 8 for the eight input variables as one vector.\n",
    "\n",
    "The other parameters for a layer or how many layers you need for a model is not an easy\n",
    "question. You may use heuristics to help you design the model, or you can refer to other people’s designs in dealing with a similar problem. Often, the best neural network structure is found through a process of trial-and-error experimentation. Generally, you need a network large enough to capture the structure of the problem but small enough to make it fast. In this example, let’s use a fully-connected network structure with three layers. Fully connected layers or dense layers are defined using the Linear class in PyTorch. It simply means an operation similar to matrix multiplication. You can specify the number of inputs as the first argument and the number of outputs as the second argument. The number of outputs is sometimes called the number of neurons or number of nodes in the layer. You also need an activation function after the layer. If not provided, you just take the output\n",
    "of the matrix multiplication to the next step, or sometimes you call it using linear activation, hence the name of the layer.\n",
    "\n",
    "In this example, you will use the rectified linear unit activation function, referred to as ReLU, on the first two layers and the sigmoid function in the output layer.\n",
    "A sigmoid on the output layer ensures the output is between 0 and 1, which is easy to map to either a probability of class 1 or snap to a hard classification of either class by a cut-off threshold of 0.5. In the past, you might have used sigmoid and tanh activation functions for all layers, but it turns out that sigmoid activation can lead to the problem of vanishing gradient in deep neural networks, and ReLU activation is found to provide better performance in terms of both speed and accuracy.\n",
    "\n",
    "You can piece it all together by adding each layer such that:\n",
    "\n",
    "- The model expects rows of data with 8 variables (the first argument at the first layer set to 8)\n",
    "- The first hidden layer has 12 neurons, followed by a ReLU activation function\n",
    "- The second hidden layer has 8 neurons, followed by another ReLU activation function\n",
    "- The output layer has one neuron, followed by a sigmoid activation function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f203073c8b45285"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d499572826e0e92"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:14.904811900Z",
     "start_time": "2023-08-21T01:14:14.873598100Z"
    }
   },
   "id": "4f389ced71064fba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this approach, a class needs to have all the layers defined in the constructor because you need to prepare all its components when it is created, but the input is not yet provided. Note that you also need to call the parent class’s constructor (the line `super().__init__()`) to bootstrap your model. You also need to define a `forward()` function in the class to tell, if an input tensor x is provided, how you produce the output tensor in return.\n",
    "\n",
    "Once you decide on the loss function, you also need an optimizer. The optimizer is the\n",
    "algorithm you use to adjust the model weights progressively to produce a better output.\n",
    "There are many optimizers to choose from, and in this example, Adam is used. This popular\n",
    "version of gr adient descent can automatically tune itself and gives good results in a wide range of problems.\n",
    "\n",
    "The optimizer usually has some configuration parameters. Most notably, the learning rate\n",
    "lr. But all optimizers need to know what to optimize. Therefore. you pass on model.parameters(), which is a generator of all parameters from the model you created."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "767ad0a8773c61cc"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss() # Binary cross entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:14.904811900Z",
     "start_time": "2023-08-21T01:14:14.889186100Z"
    }
   },
   "id": "230b24185c49836d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You have defined your model, the loss metric, and the optimizer. It is ready for training by executing the model on some data.\n",
    "\n",
    "Training a neural network model usually takes in epochs and batches. They are idioms for\n",
    "how data is passed to a model:\n",
    "\n",
    "- Epoch: Passes the entire training dataset to the model once\n",
    "- Batch: One or more samples passed to the model, from which the gradient descent\n",
    "algorithm will be executed for one iteration.\n",
    "\n",
    "The size of a batch is limited by the system’s memory. Also, the number of computations\n",
    "required is linearly proportional to the size of a batch. The total number of batches over\n",
    "many epochs is how many times you run the gradient descent to refine the model. It is a\n",
    "trade off that you want more iterations for the gradient descent so you can produce a better model, but at the same time, you do not want the training to take too long to complete. The number of epochs and the size of a batch can be chosen experimentally by trial and error.\n",
    "\n",
    "The goal of training a model is to ensure it learns a good enough mapping of input data to\n",
    "output classification. It will not be perfect, and errors are inevitable. Usually, you will see the amount o f error reducing when in the later epochs, but it will eventually level out. This is called model convergence."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb8b70c12b7bbd1c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.6111011505126953\n",
      "Finished epoch 1, latest loss 0.5506048798561096\n",
      "Finished epoch 2, latest loss 0.5479680299758911\n",
      "Finished epoch 3, latest loss 0.5462202429771423\n",
      "Finished epoch 4, latest loss 0.5351380109786987\n",
      "Finished epoch 5, latest loss 0.5223537683486938\n",
      "Finished epoch 6, latest loss 0.5195610523223877\n",
      "Finished epoch 7, latest loss 0.5059084892272949\n",
      "Finished epoch 8, latest loss 0.5092824697494507\n",
      "Finished epoch 9, latest loss 0.5084242224693298\n",
      "Finished epoch 10, latest loss 0.5107033848762512\n",
      "Finished epoch 11, latest loss 0.5091471076011658\n",
      "Finished epoch 12, latest loss 0.5129774808883667\n",
      "Finished epoch 13, latest loss 0.5170820951461792\n",
      "Finished epoch 14, latest loss 0.5171381235122681\n",
      "Finished epoch 15, latest loss 0.5113753080368042\n",
      "Finished epoch 16, latest loss 0.5145292282104492\n",
      "Finished epoch 17, latest loss 0.5092077255249023\n",
      "Finished epoch 18, latest loss 0.49967071413993835\n",
      "Finished epoch 19, latest loss 0.504838228225708\n",
      "Finished epoch 20, latest loss 0.49864527583122253\n",
      "Finished epoch 21, latest loss 0.495563805103302\n",
      "Finished epoch 22, latest loss 0.5035557746887207\n",
      "Finished epoch 23, latest loss 0.4922219216823578\n",
      "Finished epoch 24, latest loss 0.49489638209342957\n",
      "Finished epoch 25, latest loss 0.481976181268692\n",
      "Finished epoch 26, latest loss 0.4840907156467438\n",
      "Finished epoch 27, latest loss 0.4794609546661377\n",
      "Finished epoch 28, latest loss 0.4733390510082245\n",
      "Finished epoch 29, latest loss 0.4785223603248596\n",
      "Finished epoch 30, latest loss 0.46192467212677\n",
      "Finished epoch 31, latest loss 0.47067469358444214\n",
      "Finished epoch 32, latest loss 0.4630124568939209\n",
      "Finished epoch 33, latest loss 0.46493369340896606\n",
      "Finished epoch 34, latest loss 0.46517834067344666\n",
      "Finished epoch 35, latest loss 0.46656668186187744\n",
      "Finished epoch 36, latest loss 0.45516037940979004\n",
      "Finished epoch 37, latest loss 0.46100449562072754\n",
      "Finished epoch 38, latest loss 0.46118929982185364\n",
      "Finished epoch 39, latest loss 0.46926888823509216\n",
      "Finished epoch 40, latest loss 0.452389657497406\n",
      "Finished epoch 41, latest loss 0.45867887139320374\n",
      "Finished epoch 42, latest loss 0.46677976846694946\n",
      "Finished epoch 43, latest loss 0.44143205881118774\n",
      "Finished epoch 44, latest loss 0.46402716636657715\n",
      "Finished epoch 45, latest loss 0.4539046883583069\n",
      "Finished epoch 46, latest loss 0.4423571527004242\n",
      "Finished epoch 47, latest loss 0.453728049993515\n",
      "Finished epoch 48, latest loss 0.4435175359249115\n",
      "Finished epoch 49, latest loss 0.44629716873168945\n",
      "Finished epoch 50, latest loss 0.4552786946296692\n",
      "Finished epoch 51, latest loss 0.43588826060295105\n",
      "Finished epoch 52, latest loss 0.4460691809654236\n",
      "Finished epoch 53, latest loss 0.43184393644332886\n",
      "Finished epoch 54, latest loss 0.4305822253227234\n",
      "Finished epoch 55, latest loss 0.42575687170028687\n",
      "Finished epoch 56, latest loss 0.4285318851470947\n",
      "Finished epoch 57, latest loss 0.4217849373817444\n",
      "Finished epoch 58, latest loss 0.4153526723384857\n",
      "Finished epoch 59, latest loss 0.40678873658180237\n",
      "Finished epoch 60, latest loss 0.4240858256816864\n",
      "Finished epoch 61, latest loss 0.41607585549354553\n",
      "Finished epoch 62, latest loss 0.4091089069843292\n",
      "Finished epoch 63, latest loss 0.42664456367492676\n",
      "Finished epoch 64, latest loss 0.4078558683395386\n",
      "Finished epoch 65, latest loss 0.4102252721786499\n",
      "Finished epoch 66, latest loss 0.39777126908302307\n",
      "Finished epoch 67, latest loss 0.4065198004245758\n",
      "Finished epoch 68, latest loss 0.3949474096298218\n",
      "Finished epoch 69, latest loss 0.40239641070365906\n",
      "Finished epoch 70, latest loss 0.39046940207481384\n",
      "Finished epoch 71, latest loss 0.3917402923107147\n",
      "Finished epoch 72, latest loss 0.3875024914741516\n",
      "Finished epoch 73, latest loss 0.3822574317455292\n",
      "Finished epoch 74, latest loss 0.3836000859737396\n",
      "Finished epoch 75, latest loss 0.3891448974609375\n",
      "Finished epoch 76, latest loss 0.3749900460243225\n",
      "Finished epoch 77, latest loss 0.3846006989479065\n",
      "Finished epoch 78, latest loss 0.3724671006202698\n",
      "Finished epoch 79, latest loss 0.3847501873970032\n",
      "Finished epoch 80, latest loss 0.37914445996284485\n",
      "Finished epoch 81, latest loss 0.37892869114875793\n",
      "Finished epoch 82, latest loss 0.37198418378829956\n",
      "Finished epoch 83, latest loss 0.3668607473373413\n",
      "Finished epoch 84, latest loss 0.3822786509990692\n",
      "Finished epoch 85, latest loss 0.37926602363586426\n",
      "Finished epoch 86, latest loss 0.3751007914543152\n",
      "Finished epoch 87, latest loss 0.37155887484550476\n",
      "Finished epoch 88, latest loss 0.37015917897224426\n",
      "Finished epoch 89, latest loss 0.3694930374622345\n",
      "Finished epoch 90, latest loss 0.36629632115364075\n",
      "Finished epoch 91, latest loss 0.36704719066619873\n",
      "Finished epoch 92, latest loss 0.366792231798172\n",
      "Finished epoch 93, latest loss 0.3672466576099396\n",
      "Finished epoch 94, latest loss 0.36391252279281616\n",
      "Finished epoch 95, latest loss 0.36395642161369324\n",
      "Finished epoch 96, latest loss 0.36333349347114563\n",
      "Finished epoch 97, latest loss 0.356317400932312\n",
      "Finished epoch 98, latest loss 0.36073943972587585\n",
      "Finished epoch 99, latest loss 0.36244910955429077\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T01:14:19.139737400Z",
     "start_time": "2023-08-21T01:14:14.904811900Z"
    }
   },
   "id": "f080ba2ef175e4c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
